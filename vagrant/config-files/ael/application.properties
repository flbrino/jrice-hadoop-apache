# suppress inspection "SpringBootApplicationProperties" for whole file

# Build Info (Generated by Build Process - Do Not Change)
# ----------------------------------------------------------------------------------------------------
info.build.version=8.1.0.0-212

# Daemon Server Configuration
# ----------------------------------------------------------------------------------------------------
# External server port (Spoon, Pentaho Server)
# Port for unencrypted communication with AEL, or use -1 to disable.
ael.unencrypted.port=53000

# WebSocket URL acessible by Spark driver
# Note: If SSL is enabled for Driver to Daemon communication use wss://
websocketURL=ws://localhost:${ael.unencrypted.port}

# SSL Settings
# If enabled, the key-store and key-store-type setting must
# be specified.  The password properties below can be set
# in cleartext within this property file.  If omitted, they
# will be prompted for during the startup of the daemon, after
# which they will be stored encrypted in the file enc-ssl.properties
ael.ssl.enabled=false
#ael.ssl.port=53001
#ael.ssl.key-store=/path/to/keystore
#ael.ssl.key-store-type=PKCS12
#ael.ssl.key-store-password=changeit
#ael.ssl.key-password=changeit


# Security Properties
# ----------------------------------------------------------------------------------------------------
# Name of keytab and principal used for launching the Spark application.
# If the proxy user capability is enabled below, then the principal specified
# must have permission to proxy other users.
keytabLocation=
kerberosPrincipal=

# Set to true in order to disable the proxy user.
# Disabling will cause the Spark application to run as the principal user.
disableProxyUser=false

# Server keytab location & principal settings
# By default the server principal and driver principal are set to use the
# same keytab specified above.
# Note that the principal used for the daemon server process must be
# a *service* principal of the form:
#   HTTP/servername:port@MYREALM
#
# This same principal can be used for each of the 3 security principals
# (kerberosPrincipal, http.security.principal, and driver.security.principal)
# provided the HTTP principal has sufficient permissions for executing
# on the cluster.
#http.security.keytabLocation=${keytabLocation}
#http.security.principal=${kerberosPrincipal}
#http.security.debug=false

# Driver keytab location & principal settings
#driver.security.keytabLocation=${keytabLocation}
#driver.security.principal=${kerberosPrincipal}

# Hadoop Configuration
# ----------------------------------------------------------------------------------------------------
# Location where *-site.xml files reside for Hadoop configuration
hadoopConfDir=/opt/hadoop-2.8.1/etc/hadoop

# Hadoop user that will run job
hadoopUser=devuser


# Spark Configuration
# ----------------------------------------------------------------------------------------------------
# Location of Spark client distribution\
sparkHome=/opt/spark-2.2.0-bin-hadoop2.7

# Spark Master
# Location where Spark will run: local / yarn
# Note: Simulate multiple executors by using (i.e. 2 executors):  local[2]
# local[*] will run with as many executors as logical cores on your machine
# Transformations that execute sub-transformations (i.e. Kafka Consumer) require at least 2 executors
sparkMaster=local[*]

# Deploy Mode
# Only used when the master is set to `yarn`.
# client  - Driver will run on the daemon machine; but the executors will run in Yarn
# cluster - Entire application will be run in the cluster (Not supported yet)
sparkDeployMode=client


# AEL Spark Properties
# ----------------------------------------------------------------------------------------------------
# Main PDI driver location (can be pdi-ee-client or pdi-spark-driver)
sparkApp=/home/vagrant/ael/latest/data-integration

sparkAppClass=org.pentaho.pdi.spark.driver.SparkWebSocketMain

# Spark assembly zip
# The zip file of the PDI assembly used on the executors.
# It is recommended to leverage an HDFS reference to help with performance launching the application.
assemblyZip=hdfs://localhost:9000/opt/pentaho/pdi-spark-executor.zip


# Easy Access Log Settings
# ----------------------------------------------------------------------------------------------------
# See https://docs.spring.io/spring-boot/docs/current/reference/html/howto-logging.html
# Set all org.* packages to DEBUG logging
#logging.level.org=DEBUG


# Spark Debug Properties
# ----------------------------------------------------------------------------------------------------
# If set > -1 Debugging will be enabled for the Spark application.
driverDebugPort=53001
executorDebugPort=53002

# If true the Spark application will wait for a debugger to attach before executing.
suspendDebug=false

# Add extra Java options for the Spark driver application.
sparkDriverExtraJavaOptions=-Dlog4j.configuration=file:${sparkApp}/classes/log4j.xml -agentpath:/opt/jrebel/lib/libjrebel64.so -Drebel.remoting_plugin=true -Drebel.remoting_port=53003
#Uncomment to run Yourkit
#sparkDriverExtraJavaOptions=-Dlog4j.configuration=file:${sparkApp}/classes/log4j.xml -agentpath:/opt/jrebel/lib/libjrebel64.so -Drebel.remoting_plugin=true -Drebel.remoting_port=53003 -agentpath:/opt/YourKit/bin/linux-aarch64/libyjpagent.so=port=53004

# Add extra Java options for the Spark executors.
sparkExecutorExtraJavaOptions=

# The amount of memory to allocate to the Spark driver (1g, 2g, 8g, ...).
sparkDriverMemory=4g

# The amount of memory to allocate to the Spark executor (1g, 2g, 8g, ...).
sparkExecutorMemory=1g

# Set to false if you need to debug the SPARK_HOME/kettleConf directory.
overwriteConfig=true

# How long the server will wait for the driver to establish a connection before timing out (30000ms, 30s, ...).
driver.session.timeout=60s

# How long Driver Session can live before timeout
driver.session.maxLiving.timeout=1h
# How long before timeout idle Driver Session
driver.session.maxIdle.timeout=15m
# Max Thread per driver session.
# Set to 0 or Less for a thread pool that creates new threads as needed, but will reuse previously constructed threads when they are available.
# Set to 1 for an Executor that uses a single worker thread operating off an unbounded queue.
# Set to 2 or more for a thread pool that reuses a fixed number of threads operating off a shared unbounded queue. At any point, at most the defined number of threads will be active processing tasks.
driver.session.maxRequestThreads=-1

# Spark History settings - enables event logging so that transformations can be viewed in the Spark History UI
# ----------------------------------------------------------------------------------------------------
# See:
#   - https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-history-server.html
#   - https://spark.apache.org/docs/latest/monitoring.html
#   - https://www.cloudera.com/documentation/enterprise/5-9-x/topics/operation_spark_applications.html
# To configure spark history, set the following properties:
#   - sparkEventLogEnabled (default: false)- set tp true to enable event logging for use with the History Server.
#     If set to false, History Server will not be enabled
#   - sparkEventLogDir     - the directory to log events to (see documentation for details)
# NOTE:  the value set for "sparkEventLogDir" should be the same as the value of the Spark History Server
#        property:  "spark.history.fs.logDirectory".  See documentation above for more details.
sparkEventLogEnabled=true
sparkEventLogDir=hdfs://localhost:9000/user/spark/spark2ApplicationHistory
